{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-15T14:02:31.102087Z","iopub.status.busy":"2024-03-15T14:02:31.101143Z","iopub.status.idle":"2024-03-15T14:02:31.106301Z","shell.execute_reply":"2024-03-15T14:02:31.105269Z","shell.execute_reply.started":"2024-03-15T14:02:31.102052Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import math\n","from openai import OpenAI\n","from dotenv import load_dotenv\n","import os\n","load_dotenv()\n","api_key = os.getenv(\"OPENAI_API_KEY\")"]},{"cell_type":"markdown","metadata":{},"source":["## Generate documents using OpenAI API"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def generate_document(prompt):\n","    client = OpenAI(\n","        api_key=api_key,\n","    )\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages,\n","        temperature=0,\n","    )\n","    return response.choices[0].message.content"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cars have become an essential part of modern society, providing individuals with a convenient and efficient mode of transportation. From commuting to work, running errands, or embarking on road trips, cars play a crucial role in our daily lives.\n","\n","The history of cars dates back to the late 19th century, with the invention of the first gasoline-powered automobile by Karl Benz in 1885. Since then, cars have evolved significantly in terms of design, technology, and performance. Today, there are a wide variety of cars available on the market, ranging from compact sedans to luxury SUVs and electric vehicles.\n","\n","One of the key benefits of owning a car is the freedom and flexibility it provides. With a car, individuals can travel to their desired destinations at their own pace and convenience, without having to rely on public transportation schedules. Cars also offer a sense of independence and autonomy, allowing individuals to explore new places and embark on spontaneous adventures.\n","\n","In addition to convenience, cars also play a crucial role in the economy. The automotive industry is a major contributor to global GDP, providing millions of jobs and driving innovation in technology and engineering. The production and sale of cars also generate significant revenue for governments through taxes and tariffs.\n","\n","However, it is important to acknowledge the environmental impact of cars. The combustion of fossil fuels in traditional gasoline-powered cars contributes to air pollution and greenhouse gas emissions, which have detrimental effects on the environment and public health. As a result, there has been a growing shift towards electric vehicles and other sustainable transportation options to reduce carbon emissions and combat climate change.\n","\n","Overall, cars have revolutionized the way we travel and have become an integral part of modern society. While they offer numerous benefits in terms of convenience and mobility, it is important to consider the environmental implications and work towards sustainable transportation solutions for the future.\n"]}],"source":["doc1 = generate_document(\"Write a doucment about cars\")\n","print(doc1)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Math and science are two fundamental subjects that play a crucial role in our understanding of the world around us. Both disciplines are interconnected and rely on each other to explain and solve complex problems.\n","\n","Mathematics is the language of science, providing the tools and techniques necessary to analyze and interpret data. It is a universal language that allows scientists to communicate and collaborate across different fields. From calculating the trajectory of a rocket to predicting the spread of a virus, math is essential in making informed decisions and solving real-world problems.\n","\n","Science, on the other hand, is the systematic study of the natural world through observation, experimentation, and analysis. It encompasses a wide range of disciplines, including biology, chemistry, physics, and earth sciences. Science helps us understand the laws of nature and how they govern the universe, from the smallest particles to the largest galaxies.\n","\n","The relationship between math and science is symbiotic, with each discipline informing and enhancing the other. For example, math is used in science to model and simulate complex systems, such as climate patterns or genetic mutations. In turn, science provides the real-world data and observations that math can analyze and interpret.\n","\n","Together, math and science have revolutionized our understanding of the world and led to countless technological advancements. From the development of vaccines to the exploration of outer space, these disciplines have shaped our modern society and continue to push the boundaries of human knowledge.\n","\n","In conclusion, math and science are essential tools for understanding the world around us and solving the challenges we face. By combining the analytical power of math with the empirical evidence of science, we can unlock the mysteries of the universe and make meaningful contributions to society.\n"]}],"source":["doc2 = generate_document(\"Write a doucment about math and science\")\n","print(doc2)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Machine learning is a branch of artificial intelligence that focuses on the development of algorithms and models that allow computers to learn from and make predictions or decisions based on data. It is a rapidly growing field with applications in a wide range of industries, including healthcare, finance, marketing, and more.\n","\n","One of the key concepts in machine learning is the idea of training a model on a dataset. This involves feeding the model with a set of input data, along with the correct output or label for each data point. The model then learns to make predictions or decisions based on this training data, and can be used to make predictions on new, unseen data.\n","\n","There are several different types of machine learning algorithms, including supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the model is trained on labeled data, where the correct output is provided for each input. This type of learning is commonly used for tasks such as classification and regression.\n","\n","Unsupervised learning, on the other hand, involves training the model on unlabeled data, where the goal is to find patterns or structure in the data. This type of learning is often used for tasks such as clustering and dimensionality reduction.\n","\n","Reinforcement learning is a type of learning where the model learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. This type of learning is commonly used in tasks such as game playing and robotics.\n","\n","Machine learning has a wide range of applications, including image and speech recognition, natural language processing, recommendation systems, and more. It has the potential to revolutionize many industries by automating tasks, improving decision-making, and enabling new capabilities.\n","\n","In conclusion, machine learning is a powerful tool that is transforming the way we use and interact with data. By training models on large datasets, we can create systems that can learn from experience and make intelligent decisions. As the field continues to advance, we can expect to see even more exciting applications of machine learning in the future.\n"]}],"source":["doc3 = generate_document(\"Write a doucment about machine learning\")\n","print(doc3)"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def preprocess(text):\n","    def convert_to_lower(text):\n","        copy_text = text\n","        return copy_text.lower()\n","\n","    text = convert_to_lower(text)\n","    import string\n","\n","    def remove_punctuations(text):\n","        copy_text = text\n","        punc = string.punctuation\n","        return copy_text.translate(str.maketrans(\"\", \"\", punc))\n","\n","    text = remove_punctuations(text)\n","    from nltk.corpus import stopwords\n","\n","    STOPWORDS = stopwords.words(\"english\")\n","\n","    def remove_stopwords(text):\n","        copy_text = text\n","        copy_text = \" \".join([word for word in text.split() if word not in STOPWORDS])\n","        return copy_text\n","\n","    text = remove_stopwords(text)\n","    import re\n","\n","    def remove_special_chars(text):\n","        copy_text = text\n","        copy_text = re.sub(\"[^a-zA-Z0-9]\", \" \", copy_text)\n","        copy_text = re.sub(\"\\s+\", \" \", copy_text)\n","        return copy_text\n","\n","    text = remove_special_chars(text)\n","    from nltk import pos_tag\n","    from nltk.corpus import wordnet\n","    from nltk.stem import WordNetLemmatizer\n","\n","    lemmatizer = WordNetLemmatizer()\n","\n","    wordnet_map = {\n","        \"N\": wordnet.NOUN,\n","        \"V\": wordnet.VERB,\n","        \"J\": wordnet.ADJ,\n","        \"R\": wordnet.ADV,\n","    }\n","\n","    def lemmatize_words(text):\n","        copy_text = text\n","        # find pos tags\n","        pos_text = pos_tag(copy_text.split())\n","        # print(pos_text)\n","        # print(pos_text)\n","        return \" \".join(\n","            [\n","                lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN))\n","                for word, pos in pos_text\n","            ]\n","        )\n","\n","    text = lemmatize_words(text)\n","    import spacy\n","\n","    def lemmatize_spacy(text):\n","        # Load the English language model\n","        nlp = spacy.load(\"en_core_web_sm\")\n","\n","        # Process the text with spaCy\n","        doc = nlp(text)\n","\n","        # Lemmatize each token in the processed text\n","        lemmatized_text = \" \".join([token.lemma_ for token in doc])\n","        return lemmatized_text\n","\n","    # Print the lemmatized text\n","    text = lemmatize_spacy(text)\n","\n","    def remove_url(text):\n","        copy_text = text\n","        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", copy_text)\n","\n","    text = remove_url(text)\n","\n","    def remove_html_tags(text):\n","        copy_text = text\n","        return re.sub(r\"<.*?>\", \"\", copy_text)\n","\n","    text = remove_html_tags(text)\n","\n","    def remove_digits(text):\n","        copy_text = text\n","        return \"\".join([i for i in copy_text if not i.isdigit()])\n","\n","    text = remove_digits(text)\n","\n","    return text"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["'car become essential part modern society provide individual convenient efficient mode transportation commute work run errand embark road trip car play crucial role daily live history car date back late th century invention first gasolinepowere automobile karl benz  since car evolve significantly term design technology performance today wide variety car available market range compact sedan luxury suvs electric vehicle one key benefit own car freedom flexibility provide car individual travel desire destination pace convenience without rely public transportation schedule car also offer sense independence autonomy allow individual explore new place embark spontaneous adventure addition convenience car also play crucial role economy automotive industry major contributor global gdp provide million job drive innovation technology engineering production sale car also generate significant revenue government tax tariff however important acknowledge environmental impact car combustion fossil fuel traditional gasolinepowere car contribute air pollution greenhouse gas emission detrimental effect environment public health result grow shift towards electric vehicle sustainable transportation option reduce carbon emission combat climate change overall car revolutionize way travel become integral part modern society offer numerous benefit term convenience mobility important consider environmental implication work towards sustainable transportation solution future'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["text1 = preprocess(doc1)\n","text1"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["'math science two fundamental subject play crucial role understand world around u discipline interconnect rely explain solve complex problem mathematic language science provide tool technique necessary analyze interpret datum universal language allow scientist communicate collaborate across different field calculate trajectory rocket predict spread virus math essential make informed decision solve realworld problem science hand systematic study natural world observation experimentation analysis encompass wide range discipline include biology chemistry physic earth science science help u understand law nature govern universe small particle large galaxy relationship math science symbiotic discipline inform enhance example math use science model simulate complex system climate pattern genetic mutation turn science provide realworld data observation math analyze interpret together math science revolutionize understand world lead countless technological advancement development vaccine exploration outer space discipline shape modern society continue push boundary human knowledge conclusion math science essential tool understand world around u solve challenge face combine analytical power math empirical evidence science unlock mystery universe make meaningful contribution society'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["text2 = preprocess(doc2)\n","text2"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'machine learn branch artificial intelligence focus development algorithm model allow computer learn make prediction decision base datum rapidly grow field application wide range industry include healthcare finance market one key concept machine learn idea training model dataset involve feed model set input datum along correct output label datum point model learn make prediction decision base training datum use make prediction new unseen datum several different type machine learn algorithm include supervise learn unsupervised learning reinforcement learn supervise learn model train label datum correct output provide input type learn commonly use task classification regression unsupervise learning hand involve train model unlabeled datum goal find pattern structure datum type learning often use task cluster dimensionality reduction reinforcement learn type learn model learn make decision interact environment receive feedback form reward penalty type learn commonly use task game play robotic machine learn wide range application include image speech recognition natural language processing recommendation system potential revolutionize many industry automate task improve decisionmake enable new capability conclusion machine learn powerful tool transform way use interact datum training model large dataset create system learn experience make intelligent decision field continue advance expect see even excite application machine learn future'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["text3 = preprocess(doc3)\n","text3"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare the documents to calculate TF, IDF, TF-IDF"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["323"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["all_words = []\n","all_words.extend(text1.split())\n","all_words.extend(text2.split())\n","all_words.extend(text3.split())\n","unique_words = set(all_words)\n","unique_words = list(unique_words)\n","unique_words = [word for word in unique_words if len(word) >= 2]\n","len(unique_words)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["543"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(all_words)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["323"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(unique_words)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T14:02:31.112009Z","iopub.status.busy":"2024-03-15T14:02:31.111718Z","iopub.status.idle":"2024-03-15T14:02:31.118806Z","shell.execute_reply":"2024-03-15T14:02:31.117902Z","shell.execute_reply.started":"2024-03-15T14:02:31.111984Z"},"trusted":true},"outputs":[],"source":["all_docs = [\n","    text1,\n","    text2,\n","    text3,\n","]\n","# all_docs = [\n","#     \"the cats are in the house\",\n","#     \"the dogs are in the house and outside\",\n","#     \"the cats and dogs are friends\",\n","# ]"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# unique_words = []\n","# for i in all_docs:\n","#     unique_words.extend(i.split())\n","#     unique_words = list(set(unique_words))\n","# len(unique_words)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def sort_df(df):\n","    sorted_matrix = df.sort_index(axis=1)\n","    return sorted_matrix"]},{"cell_type":"markdown","metadata":{},"source":["## Calculating TF (Term Frequency)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def get_tf(all_docs, unique_words):\n","    len_docs = len(all_docs)\n","    len_words = len(unique_words)\n","    tf = np.zeros((len_docs, len_words), dtype=int)\n","    for i in range(len_docs):\n","        for j in range(len_words):\n","            cur_word = unique_words[j]\n","            cur_doc = all_docs[i]\n","            freq = cur_doc.count(cur_word)\n","            tf[i, j] = freq\n","    return tf"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["(3, 323)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tf = get_tf(all_docs, unique_words)\n","tf.shape"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acknowledge</th>\n","      <th>across</th>\n","      <th>addition</th>\n","      <th>advance</th>\n","      <th>advancement</th>\n","      <th>adventure</th>\n","      <th>air</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>along</th>\n","      <th>...</th>\n","      <th>use</th>\n","      <th>vaccine</th>\n","      <th>variety</th>\n","      <th>vehicle</th>\n","      <th>virus</th>\n","      <th>way</th>\n","      <th>wide</th>\n","      <th>without</th>\n","      <th>work</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 323 columns</p>\n","</div>"],"text/plain":["   acknowledge  across  addition  advance  advancement  adventure  air  \\\n","0            1       0         1        0            0          1    1   \n","1            0       1         0        1            1          0    0   \n","2            0       0         0        1            0          0    0   \n","\n","   algorithm  allow  along  ...  use  vaccine  variety  vehicle  virus  way  \\\n","0          0      1      0  ...    1        0        1        2      0    1   \n","1          0      1      0  ...    1        1        0        0      1    0   \n","2          2      1      1  ...    5        0        0        0      0    1   \n","\n","   wide  without  work  world  \n","0     1        1     2      0  \n","1     1        0     0      6  \n","2     2        0     0      0  \n","\n","[3 rows x 323 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["df_tf = pd.DataFrame(tf, columns=unique_words)\n","df_tf = sort_df(df_tf)\n","df_tf"]},{"cell_type":"markdown","metadata":{},"source":["## Calculating IDF (Inverse Document Frequency)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def get_freq_all(word, all_docs):\n","    counter = 0\n","    for i in all_docs:\n","        if word in i.split():\n","            counter += 1\n","    return counter"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def get_idf(all_docs, unique_words):\n","    len_docs = len(all_docs)\n","    len_words = len(unique_words)\n","    idf = np.zeros((len_words))\n","    for i in range(len_words):\n","        freq = get_freq_all(unique_words[i], all_docs)\n","        idf[i] = math.log(float(len_docs + 1) / float(freq + 1)) + 1\n","    return idf"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["(323,)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["idf = get_idf(all_docs, unique_words)\n","idf.shape"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acknowledge</th>\n","      <th>across</th>\n","      <th>addition</th>\n","      <th>advance</th>\n","      <th>advancement</th>\n","      <th>adventure</th>\n","      <th>air</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>along</th>\n","      <th>...</th>\n","      <th>use</th>\n","      <th>vaccine</th>\n","      <th>variety</th>\n","      <th>vehicle</th>\n","      <th>virus</th>\n","      <th>way</th>\n","      <th>wide</th>\n","      <th>without</th>\n","      <th>work</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.0</td>\n","      <td>1.693147</td>\n","      <td>...</td>\n","      <td>1.287682</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.287682</td>\n","      <td>1.0</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows × 323 columns</p>\n","</div>"],"text/plain":["   acknowledge    across  addition   advance  advancement  adventure  \\\n","0     1.693147  1.693147  1.693147  1.693147     1.693147   1.693147   \n","\n","        air  algorithm  allow     along  ...       use   vaccine   variety  \\\n","0  1.693147   1.693147    1.0  1.693147  ...  1.287682  1.693147  1.693147   \n","\n","    vehicle     virus       way  wide   without      work     world  \n","0  1.693147  1.693147  1.287682   1.0  1.693147  1.693147  1.693147  \n","\n","[1 rows x 323 columns]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["df_idf = pd.DataFrame(idf, index=unique_words)\n","df_idf = sort_df(df_idf.T)\n","df_idf"]},{"cell_type":"markdown","metadata":{},"source":["## Calculating TF-IDF"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def get_tfidf(tf, idf):\n","    tf_idf = idf * tf\n","    normalization = np.sqrt((tf_idf * tf_idf).sum(axis=1))\n","    for i in range(len(normalization)):\n","        tf_idf[i] /= normalization[i]\n","    return tf_idf"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["(3, 323)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["tf_idf = get_tfidf(tf, idf)\n","tf_idf.shape"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acknowledge</th>\n","      <th>across</th>\n","      <th>addition</th>\n","      <th>advance</th>\n","      <th>advancement</th>\n","      <th>adventure</th>\n","      <th>air</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>along</th>\n","      <th>...</th>\n","      <th>use</th>\n","      <th>vaccine</th>\n","      <th>variety</th>\n","      <th>vehicle</th>\n","      <th>virus</th>\n","      <th>way</th>\n","      <th>wide</th>\n","      <th>without</th>\n","      <th>work</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.04611</td>\n","      <td>0.000000</td>\n","      <td>0.04611</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.04611</td>\n","      <td>0.04611</td>\n","      <td>0.000000</td>\n","      <td>0.027234</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.035068</td>\n","      <td>0.000000</td>\n","      <td>0.04611</td>\n","      <td>0.092221</td>\n","      <td>0.000000</td>\n","      <td>0.035068</td>\n","      <td>0.027234</td>\n","      <td>0.04611</td>\n","      <td>0.092221</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.00000</td>\n","      <td>0.042679</td>\n","      <td>0.00000</td>\n","      <td>0.042679</td>\n","      <td>0.042679</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.025207</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.032458</td>\n","      <td>0.042679</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.042679</td>\n","      <td>0.000000</td>\n","      <td>0.025207</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.256071</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.032755</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.065509</td>\n","      <td>0.019345</td>\n","      <td>0.032755</td>\n","      <td>...</td>\n","      <td>0.124553</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.024911</td>\n","      <td>0.038691</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 323 columns</p>\n","</div>"],"text/plain":["   acknowledge    across  addition   advance  advancement  adventure      air  \\\n","0      0.04611  0.000000   0.04611  0.000000     0.000000    0.04611  0.04611   \n","1      0.00000  0.042679   0.00000  0.042679     0.042679    0.00000  0.00000   \n","2      0.00000  0.000000   0.00000  0.032755     0.000000    0.00000  0.00000   \n","\n","   algorithm     allow     along  ...       use   vaccine  variety   vehicle  \\\n","0   0.000000  0.027234  0.000000  ...  0.035068  0.000000  0.04611  0.092221   \n","1   0.000000  0.025207  0.000000  ...  0.032458  0.042679  0.00000  0.000000   \n","2   0.065509  0.019345  0.032755  ...  0.124553  0.000000  0.00000  0.000000   \n","\n","      virus       way      wide  without      work     world  \n","0  0.000000  0.035068  0.027234  0.04611  0.092221  0.000000  \n","1  0.042679  0.000000  0.025207  0.00000  0.000000  0.256071  \n","2  0.000000  0.024911  0.038691  0.00000  0.000000  0.000000  \n","\n","[3 rows x 323 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["df_tf_idf = pd.DataFrame(tf_idf, columns=unique_words)\n","df_tf_idf = sort_df(df_tf_idf)\n","df_tf_idf"]},{"cell_type":"markdown","metadata":{},"source":["## TfidfVectorizer"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T14:02:31.402926Z","iopub.status.busy":"2024-03-15T14:02:31.402459Z","iopub.status.idle":"2024-03-15T14:02:31.409721Z","shell.execute_reply":"2024-03-15T14:02:31.408768Z","shell.execute_reply.started":"2024-03-15T14:02:31.402890Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["cv = CountVectorizer()\n","word_count_vector = cv.fit_transform(all_docs)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["323"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["len(cv.get_feature_names_out())"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 1,  0,  1,  0,  0,  1,  1,  0,  1,  0,  3,  0,  0,  0,  0,  0,\n","         0,  0,  1,  1,  1,  1,  1,  0,  2,  2,  1,  0,  0,  0,  0,  0,\n","        13,  1,  1,  0,  1,  0,  0,  1,  0,  0,  1,  0,  1,  0,  0,  1,\n","         1,  0,  0,  0,  0,  1,  0,  1,  0,  1,  3,  1,  0,  0,  0,  2,\n","         1,  0,  0,  1,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0,  1,\n","         0,  1,  1,  1,  2,  2,  2,  0,  0,  0,  1,  0,  1,  2,  1,  1,\n","         0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n","         0,  1,  1,  0,  0,  1,  1,  1,  0,  1,  0,  0,  1,  2,  1,  1,\n","         0,  1,  0,  0,  1,  1,  1,  0,  1,  0,  0,  1,  1,  0,  0,  0,\n","         1,  1,  2,  0,  0,  1,  3,  1,  0,  0,  1,  0,  1,  0,  0,  0,\n","         0,  0,  1,  0,  1,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n","         1,  1,  0,  1,  0,  0,  1,  0,  0,  0,  1,  1,  1,  0,  2,  0,\n","         0,  0,  0,  0,  1,  1,  0,  2,  0,  1,  1,  0,  0,  1,  1,  1,\n","         2,  0,  0,  0,  1,  0,  1,  2,  0,  1,  0,  0,  0,  0,  0,  0,\n","         0,  1,  3,  2,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n","         1,  1,  1,  1,  0,  1,  0,  0,  2,  1,  1,  1,  0,  0,  1,  0,\n","         1,  0,  0,  0,  1,  1,  1,  0,  1,  0,  2,  1,  0,  0,  0,  1,\n","         0,  0,  0,  0,  0,  2,  1,  0,  0,  0,  1,  0,  1,  0,  0,  2,\n","         2,  1,  1,  0,  0,  2,  1,  0,  0,  0,  0,  4,  2,  1,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  0,  1,  1,\n","         1,  2,  0],\n","       [ 0,  1,  0,  0,  1,  0,  0,  0,  1,  0,  0,  1,  1,  2,  0,  2,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  1,  0,\n","         0,  0,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  0,  1,  0,\n","         0,  2,  0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  0,  1,  0,  1,\n","         0,  1,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,  0,  4,  0,\n","         1,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  2,\n","         0,  1,  0,  1,  0,  0,  0,  1,  1,  1,  0,  1,  0,  0,  1,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,\n","         1,  0,  0,  1,  0,  0,  0,  1,  0,  0,  1,  0,  0,  1,  0,  0,\n","         0,  0,  0,  0,  1,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,\n","         1,  2,  0,  0,  0,  0,  0,  1,  0,  2,  1,  0,  1,  1,  0,  0,\n","         0,  0,  0,  0,  2,  0,  0,  8,  1,  1,  0,  0,  0,  1,  1,  1,\n","         1,  1,  1,  1,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n","         0,  1,  1,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  1,  0,  2,\n","         0,  0,  2,  0,  1,  1,  0,  2,  0,  0,  0,  0,  0,  0,  0,  1,\n","         1,  0,  0,  1,  0,  0,  0,  1,  1,  0,  0,  0, 11,  1,  0,  0,\n","         0,  0,  0,  1,  0,  0,  0,  1,  0,  1,  2,  0,  3,  1,  0,  0,\n","         1,  0,  1,  1,  0,  0,  0,  1,  1,  1,  0,  0,  0,  1,  1,  0,\n","         0,  0,  0,  1,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,\n","         0,  4,  1,  2,  0,  1,  0,  0,  0,  1,  1,  0,  0,  1,  0,  1,\n","         0,  0,  4],\n","       [ 0,  0,  0,  1,  0,  0,  0,  2,  1,  1,  0,  0,  0,  0,  3,  0,\n","         1,  1,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  1,  0,  1,\n","         0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  2,  0,  0,\n","         0,  0,  1,  1,  1,  0,  1,  0,  0,  0,  0,  0,  2,  0,  1,  0,\n","         0,  0,  2,  0,  9,  4,  1,  0,  0,  0,  0,  1,  1,  1,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,\n","         1,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  2,  1,\n","         1,  0,  0,  1,  1,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,\n","         0,  0,  1,  0,  0,  0,  1,  1,  0,  1,  0,  0,  0,  0,  1,  1,\n","         0,  0,  0,  1,  3,  0,  0,  2,  0,  0,  0,  2,  0,  1,  1,  2,\n","         0,  0,  0,  2,  0,  0,  1,  0,  2,  1,  1,  0,  0,  0, 17,  3,\n","         0,  0,  6,  0,  5,  1,  1,  0,  0,  0,  0,  0,  0,  8,  0,  0,\n","         0,  1,  0,  0,  2,  0,  0,  0,  1,  1,  0,  0,  2,  0,  0,  0,\n","         0,  0,  1,  1,  0,  0,  0,  1,  1,  0,  1,  0,  1,  0,  3,  0,\n","         1,  0,  1,  0,  0,  2,  1,  0,  1,  1,  1,  0,  1,  1,  2,  0,\n","         0,  0,  0,  1,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n","         0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n","         0,  1,  0,  0,  2,  0,  0,  0,  2,  0,  0,  4,  0,  0,  0,  0,\n","         0,  0,  0,  0,  1,  0,  0,  2,  3,  0,  1,  0,  0,  0,  0,  0,\n","         5,  0,  0,  0,  1,  0,  1,  1,  1,  5,  0,  0,  0,  0,  1,  2,\n","         0,  0,  0]], dtype=int64)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["word_count_vector.toarray()"]},{"cell_type":"markdown","metadata":{},"source":["## Compute TF "]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acknowledge</th>\n","      <th>across</th>\n","      <th>addition</th>\n","      <th>advance</th>\n","      <th>advancement</th>\n","      <th>adventure</th>\n","      <th>air</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>along</th>\n","      <th>...</th>\n","      <th>use</th>\n","      <th>vaccine</th>\n","      <th>variety</th>\n","      <th>vehicle</th>\n","      <th>virus</th>\n","      <th>way</th>\n","      <th>wide</th>\n","      <th>without</th>\n","      <th>work</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 323 columns</p>\n","</div>"],"text/plain":["   acknowledge  across  addition  advance  advancement  adventure  air  \\\n","0            1       0         1        0            0          1    1   \n","1            0       1         0        0            1          0    0   \n","2            0       0         0        1            0          0    0   \n","\n","   algorithm  allow  along  ...  use  vaccine  variety  vehicle  virus  way  \\\n","0          0      1      0  ...    0        0        1        2      0    1   \n","1          0      1      0  ...    1        1        0        0      1    0   \n","2          2      1      1  ...    5        0        0        0      0    1   \n","\n","   wide  without  work  world  \n","0     1        1     2      0  \n","1     1        0     0      4  \n","2     2        0     0      0  \n","\n","[3 rows x 323 columns]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["tf_vectorizer = word_count_vector.toarray()\n","df_count_vector = pd.DataFrame(\n","    tf_vectorizer,\n","    columns=cv.get_feature_names_out(),\n",")\n","df_count_vector = sort_df(df_count_vector)\n","df_count_vector"]},{"cell_type":"markdown","metadata":{},"source":["## Compute IDF"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acknowledge</th>\n","      <th>across</th>\n","      <th>addition</th>\n","      <th>advance</th>\n","      <th>advancement</th>\n","      <th>adventure</th>\n","      <th>air</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>along</th>\n","      <th>...</th>\n","      <th>use</th>\n","      <th>vaccine</th>\n","      <th>variety</th>\n","      <th>vehicle</th>\n","      <th>virus</th>\n","      <th>way</th>\n","      <th>wide</th>\n","      <th>without</th>\n","      <th>work</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.0</td>\n","      <td>1.693147</td>\n","      <td>...</td>\n","      <td>1.287682</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.287682</td>\n","      <td>1.0</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","      <td>1.693147</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows × 323 columns</p>\n","</div>"],"text/plain":["   acknowledge    across  addition   advance  advancement  adventure  \\\n","0     1.693147  1.693147  1.693147  1.693147     1.693147   1.693147   \n","\n","        air  algorithm  allow     along  ...       use   vaccine   variety  \\\n","0  1.693147   1.693147    1.0  1.693147  ...  1.287682  1.693147  1.693147   \n","\n","    vehicle     virus       way  wide   without      work     world  \n","0  1.693147  1.693147  1.287682   1.0  1.693147  1.693147  1.693147  \n","\n","[1 rows x 323 columns]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n","tfidf_transformer.fit(word_count_vector)\n","idf_vectorizer = tfidf_transformer.idf_\n","df_tfidf_transformer = pd.DataFrame(idf_vectorizer, index=cv.get_feature_names_out())\n","df_tfidf_transformer = sort_df(df_tfidf_transformer.T)\n","df_tfidf_transformer"]},{"cell_type":"markdown","metadata":{},"source":["## Compute tf-idf by multiplying tf, idf (built in)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acknowledge</th>\n","      <th>across</th>\n","      <th>addition</th>\n","      <th>advance</th>\n","      <th>advancement</th>\n","      <th>adventure</th>\n","      <th>air</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>along</th>\n","      <th>...</th>\n","      <th>use</th>\n","      <th>vaccine</th>\n","      <th>variety</th>\n","      <th>vehicle</th>\n","      <th>virus</th>\n","      <th>way</th>\n","      <th>wide</th>\n","      <th>without</th>\n","      <th>work</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.049806</td>\n","      <td>0.000000</td>\n","      <td>0.049806</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.049806</td>\n","      <td>0.049806</td>\n","      <td>0.00000</td>\n","      <td>0.029416</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.049806</td>\n","      <td>0.099612</td>\n","      <td>0.000000</td>\n","      <td>0.037879</td>\n","      <td>0.029416</td>\n","      <td>0.049806</td>\n","      <td>0.099612</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.030652</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.039470</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.030652</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.207593</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.03888</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.07776</td>\n","      <td>0.022963</td>\n","      <td>0.03888</td>\n","      <td>...</td>\n","      <td>0.147847</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029569</td>\n","      <td>0.045927</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 323 columns</p>\n","</div>"],"text/plain":["   acknowledge    across  addition  advance  advancement  adventure       air  \\\n","0     0.049806  0.000000  0.049806  0.00000     0.000000   0.049806  0.049806   \n","1     0.000000  0.051898  0.000000  0.00000     0.051898   0.000000  0.000000   \n","2     0.000000  0.000000  0.000000  0.03888     0.000000   0.000000  0.000000   \n","\n","   algorithm     allow    along  ...       use   vaccine   variety   vehicle  \\\n","0    0.00000  0.029416  0.00000  ...  0.000000  0.000000  0.049806  0.099612   \n","1    0.00000  0.030652  0.00000  ...  0.039470  0.051898  0.000000  0.000000   \n","2    0.07776  0.022963  0.03888  ...  0.147847  0.000000  0.000000  0.000000   \n","\n","      virus       way      wide   without      work     world  \n","0  0.000000  0.037879  0.029416  0.049806  0.099612  0.000000  \n","1  0.051898  0.000000  0.030652  0.000000  0.000000  0.207593  \n","2  0.000000  0.029569  0.045927  0.000000  0.000000  0.000000  \n","\n","[3 rows x 323 columns]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import normalize\n","tf_idf_vectorizer = tf_vectorizer * idf_vectorizer\n","tf_idf_vectorizer = normalize(tf_idf_vectorizer)\n","df_tf_idf_vectorizer = pd.DataFrame(tf_idf_vectorizer, columns=cv.get_feature_names_out())\n","df_tf_idf_vectorizer = sort_df(df_tf_idf_vectorizer)\n","df_tf_idf_vectorizer"]},{"cell_type":"markdown","metadata":{},"source":["## Compute tf-idf built in"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acknowledge</th>\n","      <th>across</th>\n","      <th>addition</th>\n","      <th>advance</th>\n","      <th>advancement</th>\n","      <th>adventure</th>\n","      <th>air</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>along</th>\n","      <th>...</th>\n","      <th>use</th>\n","      <th>vaccine</th>\n","      <th>variety</th>\n","      <th>vehicle</th>\n","      <th>virus</th>\n","      <th>way</th>\n","      <th>wide</th>\n","      <th>without</th>\n","      <th>work</th>\n","      <th>world</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.049806</td>\n","      <td>0.000000</td>\n","      <td>0.049806</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.049806</td>\n","      <td>0.049806</td>\n","      <td>0.00000</td>\n","      <td>0.029416</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.049806</td>\n","      <td>0.099612</td>\n","      <td>0.000000</td>\n","      <td>0.037879</td>\n","      <td>0.029416</td>\n","      <td>0.049806</td>\n","      <td>0.099612</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.030652</td>\n","      <td>0.00000</td>\n","      <td>...</td>\n","      <td>0.039470</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.051898</td>\n","      <td>0.000000</td>\n","      <td>0.030652</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.207593</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.03888</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.07776</td>\n","      <td>0.022963</td>\n","      <td>0.03888</td>\n","      <td>...</td>\n","      <td>0.147847</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029569</td>\n","      <td>0.045927</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 323 columns</p>\n","</div>"],"text/plain":["   acknowledge    across  addition  advance  advancement  adventure       air  \\\n","0     0.049806  0.000000  0.049806  0.00000     0.000000   0.049806  0.049806   \n","1     0.000000  0.051898  0.000000  0.00000     0.051898   0.000000  0.000000   \n","2     0.000000  0.000000  0.000000  0.03888     0.000000   0.000000  0.000000   \n","\n","   algorithm     allow    along  ...       use   vaccine   variety   vehicle  \\\n","0    0.00000  0.029416  0.00000  ...  0.000000  0.000000  0.049806  0.099612   \n","1    0.00000  0.030652  0.00000  ...  0.039470  0.051898  0.000000  0.000000   \n","2    0.07776  0.022963  0.03888  ...  0.147847  0.000000  0.000000  0.000000   \n","\n","      virus       way      wide   without      work     world  \n","0  0.000000  0.037879  0.029416  0.049806  0.099612  0.000000  \n","1  0.051898  0.000000  0.030652  0.000000  0.000000  0.207593  \n","2  0.000000  0.029569  0.045927  0.000000  0.000000  0.000000  \n","\n","[3 rows x 323 columns]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["count_vector = cv.transform(all_docs)\n","tf_idf_vector = tfidf_transformer.transform(count_vector)\n","df_tf_idf_vector = pd.DataFrame(tf_idf_vector.todense(), columns=cv.get_feature_names_out())\n","df_tf_idf_vector = sort_df(df_tf_idf_vector)\n","df_tf_idf_vector"]},{"cell_type":"markdown","metadata":{},"source":["## Check the unique words from TfidfVectorizer and from scratch"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["s1 = cv.get_feature_names_out().copy()\n","s2 = unique_words.copy()\n","s1 = sorted(s1)\n","s2 = sorted(s2)\n","s1 == s2"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["def compare(df1, df2):\n","    arr1 = df1.to_numpy()\n","    arr2 = df2.to_numpy()\n","    element_wise_comparison = np.isclose(arr1, arr2)\n","    return element_wise_comparison"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["array([[False,  True, False,  True,  True, False, False,  True, False,\n","         True, False,  True,  True,  True,  True,  True,  True,  True,\n","        False, False, False, False, False,  True, False, False, False,\n","         True,  True,  True,  True,  True, False, False, False,  True,\n","        False,  True,  True, False,  True,  True, False,  True, False,\n","         True,  True, False, False,  True,  True,  True,  True, False,\n","         True, False,  True, False, False, False,  True,  True,  True,\n","        False, False,  True,  True, False,  True,  True,  True, False,\n","        False, False, False,  True,  True,  True,  True, False,  True,\n","        False, False, False, False, False, False,  True,  True,  True,\n","        False,  True, False, False, False, False, False,  True, False,\n","         True,  True,  True,  True,  True,  True,  True, False,  True,\n","         True,  True,  True,  True,  True, False, False,  True, False,\n","        False, False, False,  True, False,  True,  True, False, False,\n","        False, False,  True, False,  True, False, False, False, False,\n","         True, False,  True,  True, False, False,  True,  True,  True,\n","        False, False, False,  True,  True, False, False, False,  True,\n","         True, False,  True, False,  True,  True,  True,  True,  True,\n","        False,  True, False, False, False, False,  True,  True,  True,\n","        False,  True,  True,  True,  True, False, False,  True, False,\n","         True,  True, False,  True,  True,  True, False, False, False,\n","         True, False,  True,  True,  True,  True,  True, False, False,\n","         True, False,  True, False, False,  True,  True, False, False,\n","        False, False,  True,  True,  True, False,  True, False, False,\n","         True, False,  True, False,  True,  True,  True,  True,  True,\n","        False, False, False,  True, False,  True,  True,  True,  True,\n","         True, False,  True,  True,  True,  True, False, False, False,\n","        False,  True, False,  True,  True, False, False, False, False,\n","         True,  True, False,  True, False,  True,  True,  True, False,\n","        False, False,  True, False,  True, False, False,  True,  True,\n","         True, False,  True,  True,  True,  True,  True, False, False,\n","         True,  True,  True, False,  True, False,  True,  True, False,\n","        False, False, False,  True,  True, False, False,  True,  True,\n","         True,  True, False, False, False,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True, False,  True,\n","        False, False,  True, False, False, False, False,  True],\n","       [ True, False,  True, False, False,  True,  True,  True, False,\n","         True,  True, False, False, False,  True, False,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        False, False,  True, False,  True,  True,  True,  True, False,\n","         True, False,  True, False,  True, False,  True, False,  True,\n","         True, False,  True,  True, False,  True,  True, False,  True,\n","        False,  True, False,  True,  True,  True,  True, False,  True,\n","        False,  True, False,  True,  True, False, False,  True,  True,\n","         True,  True,  True, False, False,  True, False,  True, False,\n","         True,  True,  True,  True,  True,  True, False,  True, False,\n","         True, False,  True,  True,  True, False,  True, False,  True,\n","        False,  True,  True,  True, False, False, False,  True, False,\n","         True,  True, False,  True,  True,  True,  True,  True, False,\n","         True,  True,  True, False,  True, False,  True,  True,  True,\n","         True,  True, False,  True,  True, False,  True,  True,  True,\n","        False,  True,  True, False,  True,  True, False,  True,  True,\n","         True,  True,  True,  True, False,  True,  True,  True, False,\n","        False,  True,  True,  True,  True,  True,  True, False, False,\n","         True,  True,  True,  True,  True, False,  True, False, False,\n","        False, False, False,  True,  True,  True,  True,  True,  True,\n","        False,  True,  True, False, False, False,  True,  True, False,\n","        False, False, False, False, False, False, False,  True,  True,\n","        False,  True,  True,  True,  True, False,  True,  True,  True,\n","        False, False, False, False,  True,  True, False,  True, False,\n","         True,  True,  True, False,  True, False,  True, False,  True,\n","         True, False,  True, False, False,  True, False,  True,  True,\n","         True,  True,  True,  True,  True, False, False,  True,  True,\n","        False,  True,  True,  True, False, False,  True,  True,  True,\n","        False, False,  True,  True,  True,  True,  True, False,  True,\n","         True,  True, False,  True, False, False,  True, False, False,\n","         True,  True, False,  True, False, False,  True,  True,  True,\n","        False, False, False,  True,  True,  True, False, False,  True,\n","         True, False,  True, False, False,  True,  True,  True,  True,\n","        False,  True,  True,  True,  True, False, False,  True, False,\n","        False, False,  True, False,  True,  True,  True, False, False,\n","         True,  True, False,  True, False,  True,  True, False],\n","       [ True,  True,  True, False,  True,  True,  True, False, False,\n","        False,  True,  True,  True,  True, False,  True, False, False,\n","         True,  True,  True,  True, False, False,  True,  True,  True,\n","         True,  True, False,  True, False, False,  True,  True,  True,\n","         True,  True, False,  True, False,  True,  True,  True,  True,\n","        False,  True,  True,  True,  True, False, False, False,  True,\n","        False,  True,  True,  True,  True,  True, False,  True, False,\n","         True,  True, False, False,  True, False, False, False,  True,\n","         True,  True,  True, False, False, False,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True, False,  True,\n","         True,  True, False,  True,  True,  True, False,  True,  True,\n","         True, False, False, False,  True,  True,  True,  True,  True,\n","        False, False, False, False, False,  True,  True, False, False,\n","         True,  True,  True,  True, False,  True, False,  True,  True,\n","         True,  True,  True,  True, False,  True,  True,  True, False,\n","        False, False, False,  True,  True,  True,  True, False, False,\n","         True,  True,  True, False, False,  True,  True, False,  True,\n","         True,  True, False,  True, False, False, False,  True,  True,\n","         True, False,  True,  True, False,  True, False, False, False,\n","         True,  True,  True, False, False,  True,  True, False,  True,\n","        False, False, False,  True,  True,  True,  True,  True, False,\n","        False,  True,  True,  True, False,  True,  True, False,  True,\n","         True,  True, False, False,  True,  True, False,  True,  True,\n","         True,  True,  True, False, False,  True,  True,  True, False,\n","        False,  True, False, False, False, False, False,  True, False,\n","         True, False,  True,  True, False, False,  True, False, False,\n","        False,  True, False, False, False,  True,  True,  True,  True,\n","        False, False,  True, False,  True,  True,  True,  True,  True,\n","         True,  True,  True, False,  True, False, False,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        False,  True,  True, False,  True,  True, False,  True,  True,\n","         True, False,  True,  True, False,  True,  True,  True,  True,\n","         True, False,  True,  True, False,  True,  True, False, False,\n","         True, False,  True,  True,  True,  True,  True, False,  True,\n","         True,  True, False,  True, False, False, False, False,  True,\n","         True,  True,  True, False, False,  True,  True,  True]])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["x = compare(df_tf_idf, df_tf_idf_vectorizer)\n","x"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","def cos_similarity(df1, df2):\n","    array1 = df1.to_numpy()\n","    array2 = df2.to_numpy()\n","    cos_similarity = cosine_similarity(array1.reshape(1, -1), array2.reshape(1, -1))\n","    return cos_similarity"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.92420875]])"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["y = cos_similarity(df_tf_idf, df_tf_idf_vectorizer)\n","y"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
